{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "import numpy as np\n",
    "import skimage\n",
    "import skimage.transform\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "from dataset import diff_rendering_dataset\n",
    "from rendering import VolumeRenderer\n",
    "from fields import RadianceField\n",
    "from training import fit_inverse_graphics_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "data_path = \"./data/bunny\"\n",
    "cam2world = np.load(os.path.join(data_path, \"cam2world.npy\"))\n",
    "images = np.load(os.path.join(data_path, \"images.npy\"))\n",
    "\n",
    "cam2world = torch.Tensor(cam2world).to(device)\n",
    "images = torch.tensor(images).to(device)\n",
    "intrinsics = torch.tensor([[0.7, 0., 0.5],\n",
    "                            [0., 0.7, 0.5],\n",
    "                            [0., 0., 1.]]).to(device)\n",
    "print(cam2world.shape, images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset\n",
    "\n",
    "bunny_dataset = diff_rendering_dataset(images, cam2world, device=device)\n",
    "model_input, gt = next(bunny_dataset)\n",
    "\n",
    "plt.imshow(gt.view(images.shape[1], images.shape[2], 3).detach().cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "radiance_field = RadianceField(scene_rep_name=\"HybridVoxelNeuralField\", device=device).to(device)\n",
    "renderer = VolumeRenderer(near=1.5, far=4.5, n_samples=128, white_back=True, rand=False).to(device)\n",
    "fit_inverse_graphics_representation(\n",
    "    representation=radiance_field,\n",
    "    renderer=renderer,\n",
    "    data_iter=bunny_dataset,\n",
    "    img_resolution=(128, 128, 3),\n",
    "    lr=1e-3,\n",
    "    total_steps=2_001\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
